{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow  import keras\n",
    "from keras.layers import Dense,Softmax,Flatten,Embedding\n",
    "\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Machine learning is the study of computer algorithms that \\\n",
    "improve automatically through experience. It is seen as a \\\n",
    "subset of artificial intelligence. Machine learning algorithms \\\n",
    "build a mathematical model based on sample data, known as \\\n",
    "training data, in order to make predictions or decisions without \\\n",
    "being explicitly programmed to do so. Machine learning algorithms \\\n",
    "are used in a wide variety of applications, such as email filtering \\\n",
    "and computer vision, where it is difficult or infeasible to develop \\\n",
    "conventional algorithms to perform the needed tasks.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    \"splitting the text up into smaller units like words, getting rid of punctuations\"\n",
    "    \" output : list of words in the text\"\n",
    "\n",
    "    pattern  = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "\n",
    "    return pattern.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m clean_text =\u001b[43mtext_cleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(clean_text))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtext_cleaning\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33m\"\u001b[39m\u001b[33msplitting the text up into smaller units like words, getting rid of punctuations\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33m\"\u001b[39m\u001b[33m output : list of words in the text\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m pattern  = \u001b[43mre\u001b[49m.compile(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[A-Za-z]+[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw^\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]*|[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw^\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]*[A-Za-z]+[\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw^\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]*\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pattern.findall(text.lower())\n",
      "\u001b[31mNameError\u001b[39m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "clean_text =text_cleaning(text=text)\n",
    "print(len(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_table(tokens):\n",
    "    \"\"\" Lookup table  which helps to convert words to indices and indices to words\"\"\"\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "\n",
    "    for i, token in enumerate(sorted(set(tokens))):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i]= token\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id ,id_word =lookup_table(tokens=clean_text)\n",
    "print(\"Vocab size:\", len(word_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = [word_id[word] for word in clean_text]\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for  i in clean_text:\n",
    "#     print(i,word_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(id_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokens, word_to_id, window):\n",
    "    X = []\n",
    "    y = []\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        left_context = range(max(0, i - window), i)\n",
    "        right_context = range(i + 1, min(n_tokens, i + window + 1))\n",
    "        \n",
    "        for j in list(left_context) + list(right_context):\n",
    "            X.append(word_to_id[tokens[i]])   # center word index\n",
    "            y.append(word_to_id[tokens[j]])   # context word index\n",
    "    \n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_training_data(clean_text, word_id, window=2)\n",
    "\n",
    "\n",
    "embedding_dim = 10\n",
    "vocab_size = len(word_id)  # depends on your cleaned text\n",
    "\n",
    "\n",
    "# One-hot encode y (context)\n",
    "y = keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name=\"embedding\"),\n",
    "    Flatten(),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "model.fit(X, y, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get embedding weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights from the embedding layer\n",
    "embedding_weights = model.get_layer(\"embedding\").get_weights()[0]\n",
    "print(embedding_weights.shape)\n",
    "\n",
    "embedding_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Map words to embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: get embedding for the word \"machine\"\n",
    "word = \"machine\"\n",
    "word_idx = word_id[word]\n",
    "print(\"word_idx:\",word_idx )\n",
    "word_vector = embedding_weights[word_idx]\n",
    "print(f\"Embedding for '{word}':\\n\", word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = embedding_weights[word_idx]\n",
    "word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {word: embedding_weights[idx] for word, idx in word_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(word_embeddings.keys())[:N]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce dimensions (PCA or t-SNE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose top N words to visualize\n",
    "N = 30\n",
    "words = list(word_embeddings.keys())[:N]\n",
    "X = np.array([word_embeddings[w] for w in words])\n",
    "\n",
    "# First reduce with PCA \n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5, max_iter=2000)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(points, labels, title):\n",
    "    plt.figure(figsize=(26,6))\n",
    "    plt.scatter(points[:,0], points[:,1], c='skyblue')\n",
    "    for i, word in enumerate(labels):\n",
    "        plt.annotate(word, (points[i,0]+0.01, points[i,1]+0.01))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings(X_pca, words, \"Word Embeddings (PCA)\")\n",
    "plot_embeddings(X_tsne, words, \"Word Embeddings (t-SNE)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare similarity between words:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec = {}\n",
    "for idx, word in id_word.items():\n",
    "    word_to_vec[word] = embedding_weights[idx]\n",
    "\n",
    "# Example: get embedding for the word \"machine\"\n",
    "print(\"machine vector:\", word_to_vec[\"machine\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "print(\"Similarity(machine, learning):\", cosine_similarity(word_to_vec[\"machine\"], word_to_vec[\"learning\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Calculation**\n",
    "\n",
    "1. Embedding Layer\n",
    "\n",
    "Formula:\n",
    "\n",
    "params\n",
    "=\n",
    "vocab_size\n",
    "×\n",
    "embedding_dim\n",
    "params=vocab_size×embedding_dim\n",
    "\n",
    "From the table: 600 params\n",
    "\n",
    "You set embedding_dim = 10, so:\n",
    "\n",
    "600\n",
    "=\n",
    "𝑣\n",
    "𝑜\n",
    "𝑐\n",
    "𝑎\n",
    "𝑏\n",
    "_\n",
    "𝑠\n",
    "𝑖\n",
    "𝑧\n",
    "𝑒\n",
    "×\n",
    "10\n",
    "  \n",
    "⟹\n",
    "  \n",
    "𝑣\n",
    "𝑜\n",
    "𝑐\n",
    "𝑎\n",
    "𝑏\n",
    "_\n",
    "𝑠\n",
    "𝑖\n",
    "𝑧\n",
    "𝑒\n",
    "=\n",
    "60\n",
    "600=vocab_size×10⟹vocab_size=60\n",
    "\n",
    "✅ So your vocabulary size is 60 words.\n",
    "\n",
    "2. Flatten Layer\n",
    "\n",
    "Flatten has no trainable parameters.\n",
    "\n",
    "It just reshapes (None, 10) → (None, 10).\n",
    "\n",
    "So params = 0.\n",
    "\n",
    "3. Dense Layer\n",
    "\n",
    "Formula:\n",
    "\n",
    "params\n",
    "=\n",
    "(\n",
    "input_dim\n",
    "×\n",
    "output_dim\n",
    ")\n",
    "+\n",
    "bias_terms\n",
    "params=(input_dim×output_dim)+bias_terms\n",
    "\n",
    "Input dim = 10 (from embedding).\n",
    "\n",
    "Output dim = vocab_size = 60.\n",
    "\n",
    "Bias terms = 60.\n",
    "\n",
    "So:\n",
    "\n",
    "(\n",
    "10\n",
    "×\n",
    "60\n",
    ")\n",
    "+\n",
    "60\n",
    "=\n",
    "600\n",
    "+\n",
    "60\n",
    "=\n",
    "660\n",
    "(10×60)+60=600+60=660"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is input_length = 1?**\n",
    "\n",
    "In Skip-gram training, each training sample consists of one center word.\n",
    "\n",
    "That means your model sees input shaped like [center_word_index] (just a single integer).\n",
    "\n",
    "Example:\n",
    "\n",
    "If the sentence is [\"machine\", \"learning\", \"is\", \"fun\"]\n",
    "\n",
    "A sample could be (center=\"learning\", context=\"machine\")\n",
    "\n",
    "Input to the model: [word_id[\"learning\"]] → a single index → input length = 1.\n",
    "\n",
    "So the input sequence length is 1, because you’re not feeding multiple words at once — only one word per training example.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If you change input_length=2, you are no longer really doing Skip-gram. Here’s why:\n",
    "\n",
    "\n",
    "**Skip-gram**\n",
    "\n",
    "Input = 1 center word\n",
    "\n",
    "Output = 1 context word\n",
    "\n",
    "So input_length = 1\n",
    "\n",
    "(center = \"learning\") → predict (\"machine\")\n",
    "(center = \"learning\") → predict (\"is\")\n",
    "\n",
    "\n",
    "\n",
    "**CBOW (Continuous Bag of Words)**\n",
    "\n",
    "Input = multiple context words (window around the center)\n",
    "\n",
    "Output = 1 center word\n",
    "\n",
    "So input_length > 1\n",
    "\n",
    "Example (window = 2):\n",
    "\n",
    "\n",
    "(context = [\"machine\", \"is\"]) → predict (\"learning\")\n",
    "\n",
    "**summary**\n",
    "Simply setting input_length=2 in your current Skip-gram code doesn’t make it Skip-gram anymore.\n",
    "\n",
    "If you keep your training data as (center → context) pairs, then input length must stay 1.\n",
    "\n",
    "If you change it to (context → center) training pairs, then input length can be 2, 3, … depending on window size → that becomes CBOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: \"I love natural language processing\"**\n",
    "\n",
    "Window size = 2\n",
    "\n",
    "Center = \"natural\"\n",
    "\n",
    "Contexts = [\"I\", \"love\", \"language\", \"processing\"]\n",
    "\n",
    "Training pairs generated:\n",
    "\n",
    "(natural → I)\n",
    "(natural → love)\n",
    "(natural → language)\n",
    "(natural → processing)\n",
    "\n",
    "\n",
    "Here:\n",
    "\n",
    "Input length = 1 (just \"natural\")\n",
    "\n",
    "Output is one of its context words (but since training loops over them, the center word learns to predict all of them).\n",
    "\n",
    "\n",
    "**Important detail**\n",
    "\n",
    "Skip-gram doesn’t predict “all at once.” Instead:\n",
    "\n",
    "For \"natural\" as input, the training dataset contains multiple samples, one per context.\n",
    "\n",
    "So the model sees \"natural\" → \"I\", then \"natural\" → \"love\", etc.\n",
    "\n",
    "Over training, the embedding learns to place \"natural\" close in vector space to its contexts.\n",
    "\n",
    "\n",
    "You pass input of length 1 (the center word).\n",
    "\n",
    "The model learns to predict all possible surrounding words (but through multiple samples, not a single multi-output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
